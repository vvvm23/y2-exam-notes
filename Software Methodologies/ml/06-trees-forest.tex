\documentclass[../ml.tex]{subfiles}

\begin{document}
    
\subsection{Decision Tree}

Decision Trees is a Machine Learning algorithm that can perform both classification and regression tasks.

A decision tree is a tree where the internal nodes contain a question and the leaf nodes are the labels. The question, depending on the answer, should lead to different nodes. 

The dataset is further divided by each node until the subset is small enough to become a leaf node.

During training, the algorithm iterates through all the features and calculates what their leaf nodes are. We decide which is the best by calculating its Gini Impurity.

\begin{center}
    Gini Impurity = $1 - (\text{prob. of Yes})^2 - (\text{prob. of No})^2$
\end{center}

If the node itself has the lowest impurity, leave it as a leaf node and make its class the majority class. If separating results in an improvement, then pick the separation with the lowest Gini impurity value.
\\

For numeric data, the process is slightly different:

\begin{itemize}
    \item Sort examples by the feature, from low to high
    \item Calculate the average value for adjacent examples
    \item Calculate the impurity of each of these averages
    \item Use the average with lowest impurity as the cut off point
\end{itemize}
\\

For multiple choice questions, the questions can match a single categorical value, or a set of them. For example, a question could be "French?" or "French or Chinese?"
\\

We can also use decision trees for regression by simply taking the average target value of the training instances associated with a leaf node.
\\

Decision Trees have the downside of often over fitting the training set. It does have the advantage of automatic feature selection, as features that don't reduce impurity will not be selected.

\subsection{Random Forests}

Decision Trees are not flexible when classifying new samples. Random forests solve this problem by constructing a set of decision trees to form a forest of decision trees.

The steps do this are as follows:

\begin{itemize}
    \item Create a bootstrapped dataset.
        \subitem Take random samples from the training data (with replacement)
    \item Build a decision tree using the bootstrapped dataset, but only use a random subset of variables.
    \item Go back to the first step and repeat with a new bootstrapped dataset.
    \item To query the random forest, run an example on all decision trees in the forest and pick the class with the most votes.
\end{itemize}
\\

About 1/3 of the data does not end up in any given bootstrapped dataset. We can use this data to evaluate our model by using it on all other trees in the forest. Do the same for all other "out-of-bag" datasets.

\end{document}
